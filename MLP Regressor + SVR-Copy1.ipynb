{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84ee79-aa69-4c1f-9a52-e36dedbd1d58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install geopy\n",
    "\n",
    "# Import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(r\"C:\\Users\\ronch\\OneDrive\\Documents\\Applied ML\")\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9d19a-9481-4ec4-95f4-efaed35701d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning:\n",
    "# Remove '(min)' from 'Time_taken(min)' and convert it to an integer\n",
    "data['Time_taken(min)'] = data['Time_taken(min)'].str.replace(r'\\(min\\)\\s*', '', regex=True).astype(int)\n",
    "\n",
    "# Remove 'conditions' prefix from 'Weatherconditions'\n",
    "data['Weatherconditions'] = data['Weatherconditions'].str.replace('conditions ', '')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26040a4d-6bb7-4d93-b998-64f1d6fb8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5327de-41f5-4e34-97ec-0c6bfe0be89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to numeric\n",
    "data['Delivery_person_Age'] = pd.to_numeric(data['Delivery_person_Age'], errors='coerce')  # convert to int, NaNs if invalid\n",
    "data['Delivery_person_Ratings'] = pd.to_numeric(data['Delivery_person_Ratings'], errors='coerce')  # convert to float\n",
    "data['multiple_deliveries'] = pd.to_numeric(data['multiple_deliveries'], errors='coerce')  # convert to int (handle NaNs)\n",
    "\n",
    "# Convert to categorical\n",
    "categorical_columns = [\n",
    "    'Weatherconditions',\n",
    "    'Road_traffic_density',\n",
    "    'Type_of_order',\n",
    "    'Type_of_vehicle',\n",
    "    'Festival',\n",
    "    'City'\n",
    "]\n",
    "data[categorical_columns] = data[categorical_columns].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcaf903-018e-4208-b0be-085ac23f07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9ac6e-32f2-4520-924d-6a6fd02db055",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9c17d-7d3e-43fb-a904-b3325655fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values (NaN) in each column of the dataset\n",
    "missing_values = data.isna().sum()\n",
    "\n",
    "# Display the number of missing values for each column\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5792c-69e9-48ba-9765-bf418aef0d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install folium\n",
    "\n",
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# Step 1: Extract delivery location latitude and longitude columns\n",
    "delivery_data = data[['Delivery_location_latitude', 'Delivery_location_longitude']]    \n",
    "\n",
    "# Step 2: Create map centered on India\n",
    "india_center = [20.5937, 78.9629]\n",
    "map_all_delivery_locations = folium.Map(location=india_center, zoom_start=5)\n",
    "\n",
    "# Step 3: Plot all restaurant latitudes and longitudes\n",
    "for _, row in delivery_data.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['Delivery_location_latitude'], row['Delivery_location_longitude']],\n",
    "        radius=3,\n",
    "        color='red',  # red color for delivery locations\n",
    "        fill=True,\n",
    "        fill_opacity=0.6\n",
    "    ).add_to(map_all_delivery_locations)\n",
    "\n",
    "# Step 4: Save the map as an HTML file to view\n",
    "map_all_delivery_locations.save(\"map_all_delivery_locations.html\")\n",
    "print(\"âœ… Map saved as 'map_all_delivery_locations.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594650a-9a10-49a2-84e8-f198b6c9c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert negative latitudes and longitudes to positive for both restaurant and delivery locations\n",
    "data['Restaurant_latitude'] = data['Restaurant_latitude'].abs()\n",
    "data['Restaurant_longitude'] = data['Restaurant_longitude'].abs()\n",
    "data['Delivery_location_latitude'] = data['Delivery_location_latitude'].abs()\n",
    "data['Delivery_location_longitude'] = data['Delivery_location_longitude'].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd7651-4ca5-4233-943f-e3267d1123c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for distance calculation\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Function to calculate distance between restaurant and delivery location (in km)\n",
    "def calculate_distance(row):\n",
    "    restaurant_coords = (row['Restaurant_latitude'], row['Restaurant_longitude'])\n",
    "    delivery_coords = (row['Delivery_location_latitude'], row['Delivery_location_longitude'])\n",
    "    return geodesic(restaurant_coords, delivery_coords).km  # return distance in km\n",
    "\n",
    "# Apply function to create a new column for delivery radius\n",
    "data['Delivery_distance_km'] = data.apply(calculate_distance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bdc20b-b961-4402-9e8b-e2d5eff0bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396a693-7314-4ed2-8818-730f371aba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = ['Restaurant_latitude', 'Restaurant_longitude', 'Delivery_location_latitude', 'Delivery_location_longitude']\n",
    "data = data[~(data[cols_to_check] == 0).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479010da-2925-4055-bcde-73fe2b925d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vars = ['Delivery_person_Age', 'Delivery_person_Ratings', \n",
    "                  'Delivery_distance_km', 'Time_taken(min)', 'multiple_deliveries']\n",
    "\n",
    "categorical_vars = ['Vehicle_condition', 'Weatherconditions', 'Type_of_order',\n",
    "                    'Type_of_vehicle', 'Festival', 'City', 'Road_traffic_density']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12140f2c-5d5b-4bd3-b482-fd3cfdba4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, var in enumerate(numerical_vars):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.histplot(data[var].dropna(), kde=False, bins=30, color='skyblue')\n",
    "    plt.title(f'Distribution of {var}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f2c5b-f764-4b52-ad99-dbf61c4765de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Replace string \"NaN\" with actual np.nan across the entire DataFrame\n",
    "data.replace(\"NaN\", np.nan, inplace=True)\n",
    "\n",
    "# Convert all object-type columns to string, strip whitespaces, lowercase, and replace 'nan' with np.nan\n",
    "for col in data.select_dtypes(include='object').columns:\n",
    "    data[col] = data[col].astype(str).str.strip().str.lower().replace('nan', np.nan)\n",
    "\n",
    "\n",
    "# Columns with encoded string 'nan' that may still exist as categories\n",
    "cols_to_clean = ['City', 'Festival', 'Road_traffic_density']\n",
    "\n",
    "for col in cols_to_clean:\n",
    "    # Convert to string, strip whitespace, lowercase everything\n",
    "    data[col] = data[col].astype(str).str.strip().str.lower()\n",
    "    \n",
    "    # Replace 'nan' and 'none' strings with actual np.nan\n",
    "    data[col] = data[col].replace(['nan', 'none'], np.nan)\n",
    "    \n",
    "    # Optional: Convert back to category if needed\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111df26a-c55c-4c99-afd2-c673885c0edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean NaNs or string 'NaN' in the 'Type_of_vehicle' column\n",
    "data['Type_of_vehicle'] = data['Type_of_vehicle'].replace('NaN', pd.NA)  # Replace 'NaN' string with actual NaN\n",
    "data = data.dropna(subset=['Type_of_vehicle'])  # Remove rows where 'Type_of_vehicle' is NaN\n",
    "\n",
    "# Check unique values after cleaning\n",
    "print(data['Type_of_vehicle'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11db5b-7af2-4576-a719-3c308bb9ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 12))\n",
    "for i, var in enumerate(categorical_vars):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.stripplot(data=data, x=var, y='Time_taken(min)', palette='Set2', jitter=True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f'Time Taken by {var}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b0137-b86b-4e6c-980e-ba7d1e86a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_vs_time = ['Delivery_person_Age', 'Delivery_person_Ratings', \n",
    "                   'multiple_deliveries', 'Delivery_distance_km']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, var in enumerate(numeric_vs_time):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.scatterplot(data=data, x=var, y='Time_taken(min)', alpha=0.6)\n",
    "    plt.title(f'{var} vs Time Taken')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b97a5-8b3e-4733-a14a-d89dd1c90c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling and transforming/encoding\n",
    "numerical_vars = ['Delivery_person_Age', 'Delivery_person_Ratings', \n",
    "                  'Delivery_distance_km', 'multiple_deliveries']\n",
    "\n",
    "categorical_vars = ['Vehicle_condition', 'Weatherconditions', 'Type_of_order',\n",
    "                    'Type_of_vehicle', 'Festival', 'City', 'Road_traffic_density']\n",
    "\n",
    "y = data['Time_taken(min)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e244b-b9e2-4596-8276-a1abac8b1385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Strip and replace 'NaN' strings with actual np.nan\n",
    "for col in categorical_vars:\n",
    "    data[col] = data[col].astype(str).str.strip()\n",
    "    data[col] = data[col].replace('NaN', np.nan)\n",
    "\n",
    "# Encode 'Road_traffic_density' with label encoding (ordinal)\n",
    "road_traffic_order = ['Low', 'Medium', 'High', 'Jam']\n",
    "label_encoder = LabelEncoder()\n",
    "data['Rd_traffic_density'] = label_encoder.fit_transform(data['Road_traffic_density'].fillna('Missing'))\n",
    "\n",
    "\n",
    "# One-Hot Encode the rest (drop_first=True to avoid multicollinearity)\n",
    "one_hot_cols = ['Weatherconditions', 'Type_of_order', 'Type_of_vehicle', 'Festival', 'City']\n",
    "data = pd.get_dummies(data, columns=one_hot_cols, drop_first=True)\n",
    "\n",
    "# Drop the original 'Road_traffic_density' column (optional, now replaced)\n",
    "data.drop(columns='Road_traffic_density', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea60c1-a883-4a73-ae33-65ed5a722205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the numerical variables\n",
    "scaled_numericals = pd.DataFrame(scaler.fit_transform(data[numerical_vars]), \n",
    "                                 columns=numerical_vars, index=data.index)\n",
    "\n",
    "# Drop original numerical columns and replace with scaled ones\n",
    "data.drop(columns=numerical_vars, inplace=True)\n",
    "data = pd.concat([data, scaled_numericals], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31496563-8c23-4dbb-9a85-aaa3d765452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "data.drop(columns=['Weatherconditions_nan', 'City_nan', 'Festival_no'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b9027-0280-4fbc-9c46-d49ac256dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns that need to be converted from boolean to 0 and 1\n",
    "boolean_columns = [\n",
    "    'Weatherconditions_Fog', 'Weatherconditions_Sandstorms', 'Weatherconditions_Stormy', \n",
    "    'Weatherconditions_Sunny', 'Weatherconditions_Windy', \n",
    "    'Type_of_order_Drinks', 'Type_of_order_Meal', 'Type_of_order_Snack', \n",
    "    'Type_of_vehicle_electric_scooter', 'Type_of_vehicle_motorcycle', 'Type_of_vehicle_scooter', \n",
    "    'Festival_yes', 'City_semi-urban', 'City_urban'\n",
    "]\n",
    "\n",
    "# Convert boolean columns to 0 and 1\n",
    "data[boolean_columns] = data[boolean_columns].astype(int)\n",
    "\n",
    "# Check the transformation\n",
    "print(data[boolean_columns].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f579a-0d08-4f15-b0f9-4abd5a3b6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define X and y for modeling\n",
    "# Define target and features\n",
    "X = data.drop(columns=[\n",
    "    'Time_taken(min)',        # target\n",
    "    'ID',                     # unique identifier\n",
    "    'Delivery_person_ID',     # personal identifier\n",
    "    'Restaurant_latitude',\n",
    "    'Restaurant_longitude',\n",
    "    'Delivery_location_latitude',\n",
    "    'Delivery_location_longitude',\n",
    "    'Order_Date',\n",
    "    'Time_Orderd',\n",
    "    'Time_Order_picked'\n",
    "])\n",
    "\n",
    "y = data['Time_taken(min)']\n",
    "\n",
    "# Print shapes\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a800c-7b92-406a-a296-77a58a6001d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac383f-872c-4ef6-ab53-a84e035b2525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import HalvingRandomSearchCV, train_test_split\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 1) Split off full train/test\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2) Subsample 50% for hyperparameter tuning\n",
    "X_sub, _, y_sub, _ = train_test_split(\n",
    "    X_train_full, y_train_full, train_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# 3) Build pipeline: impute â†’ MLP\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('mlp', MLPRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# 4) Tiny hyperparameter space\n",
    "param_dist = {\n",
    "    'mlp__hidden_layer_sizes': [(64, 32), (128, 64)],\n",
    "    'mlp__activation': ['relu'],\n",
    "    'mlp__alpha':        loguniform(1e-4, 1e-2),\n",
    "    'mlp__learning_rate_init': loguniform(1e-4, 1e-2),\n",
    "    'mlp__batch_size':   [32],\n",
    "    'mlp__tol':          [1e-3]\n",
    "}\n",
    "\n",
    "# 5) Successiveâ€‘halving on the `mlp__max_iter` budget\n",
    "halving_mlp = HalvingRandomSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_dist,\n",
    "    resource='mlp__max_iter',    # the budget parameter name in our pipeline\n",
    "    max_resources=50,            # best candidates see 50 epochs\n",
    "    min_resources=10,            # first rung: 10 epochs\n",
    "    factor=3,                    \n",
    "    cv=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 6) Run tuning on the subsample\n",
    "halving_mlp.fit(X_sub, y_sub)\n",
    "\n",
    "# 7) Grab best pipeline and hyperparameters\n",
    "best_pipe = halving_mlp.best_estimator_\n",
    "print(\"Best hyperparams:\", halving_mlp.best_params_)\n",
    "\n",
    "# 8) Retrain best MLP on full training set with a higher epoch cap\n",
    "best_pipe.set_params(mlp__max_iter=200)\n",
    "best_pipe.fit(X_train_full, y_train_full)\n",
    "\n",
    "# 9) Final evaluation on holdâ€‘out test set\n",
    "y_pred = best_pipe.predict(X_test)\n",
    "print(\"Final Test RÂ²:\", r2_score(y_test, y_pred))\n",
    "mse  = mean_squared_error(y_test, y_pred)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test RMSE: {rmse:.2f}\")\n",
    "print(f\"Test MAE:  {mae:.2f}\")\n",
    "print(f\"Test RÂ²:   {r2:.4f}\")\n",
    "\n",
    "n       = len(y_test)\n",
    "p       = X_test.shape[1]\n",
    "adj_r2  = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "print(f\"Adjusted RÂ²:   {adj_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87873f24-07ef-4033-af30-5534144d3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy.stats import uniform\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1) Add cyclical time features (assumes `data` is your preprocessed DataFrame)\n",
    "data['order_dt'] = pd.to_datetime(\n",
    "    data['Order_Date'].str.strip() + ' ' + data['Time_Orderd'].str.strip(),\n",
    "    errors='coerce'\n",
    ")\n",
    "data['hour']     = data['order_dt'].dt.hour.fillna(0)\n",
    "data['dow']      = data['order_dt'].dt.weekday.fillna(0)\n",
    "data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)\n",
    "data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)\n",
    "data['dow_sin']  = np.sin(2 * np.pi * data['dow'] / 7)\n",
    "data['dow_cos']  = np.cos(2 * np.pi * data['dow'] / 7)\n",
    "\n",
    "# 2) Define features and target\n",
    "X = data.drop(columns=[\n",
    "    'Time_taken(min)',\n",
    "    'ID',\n",
    "    'Delivery_person_ID',\n",
    "    'Restaurant_latitude',\n",
    "    'Restaurant_longitude',\n",
    "    'Delivery_location_latitude',\n",
    "    'Delivery_location_longitude',\n",
    "    'Order_Date',\n",
    "    'Time_Orderd',\n",
    "    'Time_Order_picked',\n",
    "    'order_dt',\n",
    "    'hour',\n",
    "    'dow'\n",
    "])\n",
    "y = data['Time_taken(min)']\n",
    "\n",
    "# 3) Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4) Build pipeline: impute â†’ MLP\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('mlp', MLPRegressor(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        activation='relu',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 5) Small hyperparameter space around proven region\n",
    "param_dist = {\n",
    "    'mlp__alpha':              uniform(1e-5, 1e-3),\n",
    "    'mlp__learning_rate_init': uniform(1e-3, 2e-3),\n",
    "    'mlp__tol':                [1e-4, 1e-3],\n",
    "    'mlp__batch_size':         [32, 64],\n",
    "    'mlp__max_iter':           [80, 100, 120]\n",
    "}\n",
    "\n",
    "# 6) RandomizedSearchCV for refinement\n",
    "search = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_dist,\n",
    "    n_iter=15,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best_pipe = search.best_estimator_\n",
    "print(\"Refined best params:\", search.best_params_)\n",
    "\n",
    "# 7) Evaluate on holdâ€‘out test set\n",
    "y_pred = best_pipe.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b671e75-806b-4fb3-888c-1374d16eb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(search.best_estimator_, X, y,\n",
    "                            cv=5, scoring='r2', n_jobs=-1)\n",
    "print(\"5â€‘fold CV RÂ²: %.3f Â± %.3f\" % (cv_scores.mean(), cv_scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38225dfa-ed3d-40dc-8836-6378657c308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) If best_svr isnâ€™t defined in your current session, re-tune or reload it.\n",
    "# Hereâ€™s a quick RandomizedSearchCV to get a strong SVR:\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# SVR pipeline\n",
    "svr_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('svr', SVR())\n",
    "])\n",
    "\n",
    "# Hyperparameter space (you can adjust n_iter for speed/quality tradeâ€‘off)\n",
    "param_svr = {\n",
    "    'svr__C': loguniform(1e-2, 1e3),\n",
    "    'svr__gamma': loguniform(1e-4, 1),\n",
    "    'svr__epsilon': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "rand_svr = RandomizedSearchCV(\n",
    "    svr_pipe,\n",
    "    param_svr,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "rand_svr.fit(X_train_full, y_train_full)\n",
    "best_svr = rand_svr.best_estimator_\n",
    "print(\"Best SVR params:\", rand_svr.best_params_)\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1) Build the stacking pipeline (impute â†’ stack)\n",
    "stacking_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('stack', StackingRegressor(\n",
    "        estimators=[\n",
    "            ('svr', best_svr),\n",
    "            ('mlp', best_pipe)\n",
    "        ],\n",
    "        final_estimator=RidgeCV(),\n",
    "        passthrough=True,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 2) Fit on your training data\n",
    "stacking_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 3) Predict on your holdâ€‘out set\n",
    "y_stack = stacking_pipeline.predict(X_test)\n",
    "\n",
    "# 4) Evaluate\n",
    "rmse_stack = mean_squared_error(y_test, y_stack, squared=False)\n",
    "mae_stack  = mean_absolute_error(y_test, y_stack)\n",
    "r2_stack   = r2_score(y_test, y_stack)\n",
    "\n",
    "print(f\"Stacked RMSE: {rmse_stack:.2f}\")\n",
    "print(f\"Stacked MAE:  {mae_stack:.2f}\")\n",
    "print(f\"Stacked RÂ²:   {r2_stack:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66b0c1-a7cb-446f-a2ea-8de8ad2dcded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error, median_absolute_error\n",
    "\n",
    "# y_test and y_pred are your true & predicted delivery times:\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_test, y_stack) * 100\n",
    "medae = median_absolute_error(y_test, y_stack)\n",
    "\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"Median AE: {medae:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd434b93-67af-4454-acaf-df2f3a8abbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Compute standard RÂ²\n",
    "r2 = r2_score(y_test, y_stack)\n",
    "\n",
    "# 2. Compute adjusted RÂ²\n",
    "n = len(y_test)           # number of samples\n",
    "p = X_test.shape[1]       # number of predictors/features\n",
    "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "print(f\"RÂ²:          {r2:.4f}\")\n",
    "print(f\"Adjusted RÂ²: {adj_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce763bd-134e-4d11-9130-ffc862df5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(search.best_estimator_, X, y,\n",
    "                            cv=5, scoring='r2', n_jobs=-1)\n",
    "print(\"5â€‘fold CV RÂ²: %.3f Â± %.3f\" % (cv_scores.mean(), cv_scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c654669-f88b-44cd-89dc-633cb897455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Predicted vs. Actual\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_stack, alpha=0.4)\n",
    "plt.plot([y_test.min(), y_test.max()],\n",
    "         [y_test.min(), y_test.max()],\n",
    "         'k--', linewidth=2)\n",
    "plt.xlabel('Actual Delivery Time (min)')\n",
    "plt.ylabel('Predicted Delivery Time (min)')\n",
    "plt.title('Predicted vs. Actual Delivery Time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd78472-9f12-4610-8eed-4b438e390dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
